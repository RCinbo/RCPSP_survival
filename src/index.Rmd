---
title: "Determining what makes an RCPSP hard to solve: A survival model analysis"
output:
  bookdown::html_document2:
    toc: true
    fig_caption: true
  bookdown::pdf_book:
    toc: true
    fig_caption: true
bibliography: references.bib
site: bookdown::bookdown_site
delete_merged_file: true
---

```{r setup, echo = FALSE, message = FALSE, warning = FALSE}
library(kableExtra)
library(corrplot)
library(readxl)
library(rprojroot)
library(tidyverse)
library(zoo)
library(survival)
library(ggsurvfit)
library(knitr)
library(patchwork)
library(santoku)#chop_equal in stead of cut_number
library(intsurv)
source(find_root_file("src", "functions.R",
                      criterion = has_file("RCPSP_survival.Rproj")))
```
# Introduction

## Resource Constrained Project Scheduling Problem (RCPSP)

The Resource Constrained Project Scheduling Problem (RCPSP) is a well-known and extensively studied operations research (OR) problem.
In general, an RCPSP is a project (eg. building a house), characterized by several activities (eg. digging, building the floor, building the walls, adding electricity, installing windows,...) which all require a certain number of limited resources to be executed (eg. there are only 5 manual labourers available at the same time).
There are precedence constraints between the activities meaning that some activities cannot start before others are finished (eg; there has to be wall before we can add the windows).
The goal is to schedule the activities to minimize the total time to complete the project while respecting all precedence constraints and resource constraints.
It is an NP-hard problem which means that it cannot be solved to optimality in polynomial time.
Therefore, many algorithms (mostly branch-and-bound algorithms) have been developed over the years to be able to solve harder problems in shorter time.

Figure \@ref(fig:toy-rcpsp) shows a toy example of an RCPSP.
The top left shows the precedence constraints between activities.
It can be seen that activity three cannot start before activity 2 finishes.
The top right table shows the duration and resource use for each of the activities.
It can be seen that activity three needs two resources and that there are only two resources available in total.
This essentially means that no other activity can be performed in parallel with activity three.
The bottom figure shows the optimal solution where each activity is scheduled, respecting all precedence and resource constraints.
```{r toy-rcpsp, fig.cap = "A toy RCPSP example", out.width = "400px", echo = FALSE, include = TRUE}
knitr::include_graphics(
  path = find_root_file("rcpsp_example.png",
                        criterion = has_file("report.Rproj")),
  )
```


The aim of this paper is to develop a methodology to investigate what makes an RCPSP hard to solve for the currently existing algorithms.
The next two sections explain (1) the dataset that we will work with and (2) how one can describe the complexity of an RCPSP instance.

## Data source and description

Over the years, several RCPSP instance sets have been developed for researchers to test new algorithms.
In many papers on RCPSP algorithms, researchers illustrate the quality of a new algorithm by reporting how many instances of the set they can solve and how long it takes to solve them.

Recently, a [new website](http://solutionsupdate.ugent.be/rcpsp) was developed to make run times for different algorithms on different instance sets openly available.
Any researcher can upload their results and these results are openly available to download and compare.

In this article, we first explore the available datasets for the following instance sets which can be downloaded from [here](https://www.projectmanagement.ugent.be/research/data):

* PSPLIB dataset: although these datasets have been available for a long time, some instances still have not been solved [@kolisch1997psplib]. They are very well known and commonly referred to as "J##" where ## denote the number of activities in the network. Popular problem instance sets are J30, J60, J90 and J120.
* The Coelho-Vanhoucke (CV) dataset consists of 623 RCPSP instances. These instances are meant to be hard to solve for exact algorithms according to  @cv2020.

## RCPSP instance complexity

To describe instance complexity, many different parameters and indices are developed to approximate how hard an instance is to solve [@ELMAGHRABY1980223; @DEREYCK1996347; @HERROELEN1998279].
Over the years, the list of parameters that should measure instance complexity has grown a lot and each index / parameter may have its advantages and disadvantages.

For each instance on the [website](http://solutionsupdate.ugent.be/rcpsp), several parameters describe the network topology, complexity, and resources. Table \@ref(tab:parameter) shows a list of all parameters that are considered in this report. All parameters are briefly described and it can be noticed that some seem vert similar.

```{r parameter, echo = FALSE, message = FALSE, warning = FALSE}
parameters <- read_excel(
  path = find_root_file(
    "data", "RCPLIB (Parameters and BKS).xlsx",
    criterion = has_file("RCPSP_survival.Rproj")
  ),
  sheet = "Parameter Overview",
  skip = 2
) %>%
  rename(Category = `Paramater category`) %>%
  dplyr::select(1:3) %>%
  mutate(Category = na.locf(Category),
         Parameter = str_replace(Parameter, "#", "nb"),
         Parameter = str_replace_all(Parameter, " ", "-"),
         Parameter = str_replace_all(Parameter, fixed("("), "-"),
         Parameter = str_replace_all(Parameter, fixed(")"), "")
         )

n_nc <- sum(parameters$Category == "Network Topology")
n_ci <- sum(parameters$Category == "Complexity indictors")
n_rp <- sum(parameters$Category == "Resource parameters")
n_hc <- sum(parameters$Category == "Hardness classification")

parameters %>%
  dplyr::filter(Category %in% c("Network Topology",
                                "Complexity indictors",
                                "Resource parameters",
                                "Hardness classification")) %>%
  dplyr::select(-Category) %>%
  kable(booktabs = TRUE,
        caption = "Instance topology, complexity, resource parameters, and hardness classification.",
        col.names = c("", "")) %>%
  kableExtra::group_rows(group_label = "Network Topology",
                         start_row = 1,
                         end_row = n_nc) %>%
  kableExtra::group_rows(group_label = "Complexity indictors",
                         start_row = n_nc + 1,
                         end_row = n_nc + n_ci) %>%
  kableExtra::group_rows(group_label = "Resource parameters",
                         start_row = 1 + n_nc + n_ci,
                         end_row = n_nc + n_ci + n_rp) %>%
  kableExtra::group_rows(group_label = "Hardness classification",
                         start_row = 1 + n_nc + n_ci + n_rp,
                         end_row = n_nc + n_ci + n_rp + n_hc) %>%
  kableExtra::kable_styling(font_size = 9) %>%
  kableExtra::column_spec(1, width = "0.5in") %>%
  kableExtra::column_spec(2, width = "5.5in")
```

# Methodology

The main goal of this research is to explore which RCPSP instance parameters can best predict whether or not the instance can be solved and, if it can be solved, how long it will take to find the optimal solution. Table \@ref(tab:parameter) clearly shows that there is a myriad of possible complexity indicators and it is not known which ones can best predict instance complexity.
In contrast, new parameters and indices are still being developed without properly proving their merit (ability to predict the time to find an optimal solution) in addition to the already existing indices.

Most researchers will set a maximum calculation time to find the optimal solution for each problem instance.
If the optimal solution is not found within this time limit, the instance is right-censored.
We further hypothesize that some problem instances will be too hard to solve to optimality, no matter how much time is allowed to find an optimal solution.
Due to the complexity of some problems, for instance, solving it might require more memory than what is available on the machine that implements the algorithm.
In this case, the computer will never find the optimal solution.

Therefore, we will use cure models to model the time to find the optimal solution.
The independent variables in our model will be the complexity parameters of Table \@ref(tab:parameter).
In cure models, a fraction of the population will never experience the event of interest.
Translated to our problem setting, this means that some problem instances will never be solved to optimality.
In the remainder of this text, we will call instances that can (not) be solved to optimality, no matter how much calculation time is allowed *solvable* (*unsolvable*) instances.
In essence, this means that the true time to solve unsolvable instance is $\infty$.
In this case, *cure models* should be used to properly estimate the effect of the different complexity parameters on the time to solve until optimality.

There are two main families of cure regression models: (1) mixture cure models and (2) promotion time cure models.
The mixture cure model consists of two components. *Incidence* indicates whether the subject is susceptible (i.e. whether the problem instance is solvable). 
The *latency* represents the time until a solvable instance is solved to optimality.
While incidence is usually modelled using logistic regression, both parametric and semiparametric survival models exist for the latency part.
The promotion time cure model models the survival time as an improper survival function with a PH structure that will reach a plateau in stead of zero at infinity.
Mixture cure models allow more flexibility as the cure fraction and survival function may depend on different covariates.
The main downside is that it is harder to fit these types of models.
Promotion time cure models are easier to interpret as they maintain a proportional hazard structure.
The downside is that both the cure fraction and survival function will need to depend on the same covariates.

We will use a mixture cure model:
$$S(t|x, z) = p(z)S_u(t|x) + 1- p(z)$$
where $X$ and $Z$ are two vectors of covariates for the latency and incidence part respectively.
The incidence part models the probability of an instance being solvable. 
$$p(z) = P(B=1|Z=z)$$
where $B$ is a binary variable, indication whether the instance is solvable (1) or not (0); $B = I(T<\infty)$. 
The latency part models the conditional survival for the solvable (uncured) instances.
$$S_u(t|x) = P(T>t|X=x, B=1)$$

mixture cure models: 
* https://github.com/jrdnmdhl/flexsurvcure allows different parametric models for incidence and latency, and the parameters in the model do or do not depend on x
* smcure: Incidence : logistic model but also other GLM with
various link functions
 Latency : semiparametric Cox PH and AFT model
 Estimation : EM algorithm, variances obtained by
bootstrap

https://cran.r-project.org/web/packages/intsurv/intsurv.pdf -> cure models met variable selection (bron/referentie: https://www.ncbi.nlm.nih.gov/pmc/articles/PMC9660265/)

```{r load-cv-data, echo = FALSE, message = FALSE, warning = FALSE}
data_cv <- read_excel(
  path = find_root_file(
    "data", "RCPLIB (Parameters and BKS).xlsx",
    criterion = has_file("RCPSP_survival.Rproj")
  ),
  sheet = "CV",
  skip = 0
) %>%
  rename(nbR = `#R`) %>%
  rename(nbAct = `#Act`) %>%
  rename(nblR = `#lR`) %>%
  rename(nbhR = `#hR`)
colnames(data_cv)[2] <- "IDset"
colnames(data_cv) <- str_replace_all(colnames(data_cv), " ", "-")
colnames(data_cv) <- str_replace_all(colnames(data_cv), fixed("("), "-")
colnames(data_cv) <- str_replace_all(colnames(data_cv), fixed(")"), "")

data_cv_cv <- read_csv2(
  file = find_root_file(
    "data", "CV_CV.csv",
    criterion = has_file("RCPSP_survival.Rproj")
  ),
  skip = 6
) %>%
  as.data.frame() %>%
  dplyr::select(1:4) %>%
  mutate(author = "CV")

data_cv_wz <- read_csv2(
  file = find_root_file(
    "data", "CV_WZ.csv",
    criterion = has_file("RCPSP_survival.Rproj")
  ),
  skip = 6
) %>%
  dplyr::select(1:4) %>%
  mutate(author = "WZ")

data_cv_c <- read_csv2(
  file = find_root_file(
    "data", "CV_C.csv",
    criterion = has_file("RCPSP_survival.Rproj")
  ),
  skip = 6
) %>%
  dplyr::select(1:4) %>%
  mutate(author = "C")
data_cv_all <- data_cv_cv %>%
  rbind(data_cv_wz) %>%
  rbind(data_cv_c) %>%
  mutate(Type = as.factor(Type),
         author = factor(as.factor(author),
                         levels = c("CV", "WZ", "C")))
M <- data_cv %>%
  dplyr::select(
    parameters %>%
      dplyr::filter(Category %in%
                      c("Network Topology",
                        "Complexity indictors",
                        "Resource parameters",
                        "Hardness classification")) %>%
      pull(Parameter) &
                      where(is.numeric)) %>%
  cor()
```

Exploring the correlation between the long list of complexity indices, it is clear that many indices are highly correlated. The correlation matrices for the CV instances (Figure \@ref(fig:cv-cormat) and \@ref(fig:cv-cormat-reorder) in appendix \@ref(app-1)) show that some parameters measure almost the same thing.
To avoid multicollinearity problems, it is clear that some parameters should therefore not be added together in a statistical model.
We will apply regularization techniques to be able to work with the multicollinearity and perform variable selection.
Regularization techniques such as least absolute shrinkage and deletion operator (LASSO), adaptive LASSO (ALASSO), elastic net, and smoothly clipped absolute deviation (SCAD) minimize an objective function (eg likelihood function) that contains a penalty function to reflect sparsity.
These techniques have been less studied in cure models; a detailed overview can be found in @amicoCureModelsSurvival2018 and a more recent overview can be found in @suAnalysisSurvivalData2022.

We will use the *cox_cure_net* function in the [*intsurv*](https://cran.r-project.org/web/packages/intsurv/intsurv.pdf) package in R which allows to fit a regularized Cox cure rate model [@intsurv].
For solvable instances ($B_j=1$), we thus assume that the time to optimally solve instance $j$ follows:
$$S_u(t|B_j=1, x_j) = S_{u,0}(t)^{exp(\beta^\intercal x_j)}$$
where $\beta$ are the parameters that need to be estimated and with the baseline survival function $S_{u,0}(t|B_j=1, x_j) = P(T>t|B=1, X=0)$ left unspecified.

A logistic regression setup is used to model $p_j$:
$$p_j(z) = \frac{exp(z_j^\intercal \gamma + \gamma_0)}{1+exp(z_j^\intercal \gamma + \gamma_0)}$$
Where $\gamma_0$ and $y$ are the intercept and all other parameters that need to be estimated.

While the PH assumption remains valid for solvable instances, it will no longer be valid at the level of the population (including unsolvable instances).
Therefore, in stead of the partial likelihood approach, the expectation-maximization (EM) is used to fit the model.

The likelihood function, which will be optimized using the expectation-maximization (EM) algorithm can be written down in the following way:

$$L_c(\gamma, \beta, S_{u,0}) = \prod_{i=1}^n\{[p(Z_i)h_u(Y_i|X_i)S_u(Y_i|X_i)]^{\Delta_i B_i} * [p(Z_i)S_u(Y_i|X_i)]^{(1-\Delta_i)B_i} * [1-p(Z_i)]^{(1-\Delta_i)(1-B_i)}\} $$

where $\Delta$ is a binary variable, indicating whether the observation is right-censored ($\Delta =0$) or not ($\Delta = 1$).

As suggested in @masud2018variable, we can then use adaptive LASSO and imposed $L_1$ norm penalty on the log likelihood.
The *intsurv * implements Elastic Net which has two tuning parameters for both the incidence and latency part:

* $\alpha_i$: penalty for the incidence part of the loglikelihood function. If this is 1 (0), the elastic net reduced to LASSO (ridge) regression.
* $\alpha_l$: penalty for the latency part of the loglikelihood function. If this is 1 (0), the elastic net reduced to LASSO (ridge) regression.
* $\lambda_i$: A constant that multiplies the penalty terms for the incidence part of the loglikelihood function.
* $\lambda_l$: A constant that multiplies the penalty terms for the latency part of the loglikelihood function.


*from the package documentation: I found "lambda * (penalty_factor * alpha * lasso + (1 - alpha) / 2 * ridge)"
The penalized log likelihood can be written in the following way:


$$ pl_c(\gamma, \beta, , S_{u,0}) = log(L_c(\gamma, \beta, S_{u,0})) + \\ \lambda_c\alpha_c\sum_{j=1}^k|\gamma_j| + \frac{\lambda_c}{2}(1-\alpha_c)\sum_{j=1}^k\gamma_j^2 + \\ \lambda_u\alpha_u\sum_{j=1}^k|\beta_j| + \frac{\lambda_u}{2}(1-\alpha_u)\sum_{j=1}^k\beta_j^2$$

# Results

In this section, we describe our results. In section \@ref(subsec-exploration) we explore the data and select a subset of the data for further exploration.

------------mention something about Concordance Index (see intsurv)?------------


## Data exploration {#subsec-exploration}

We downloaded all files on the PSPLIB and CV instance sets. We only consider exact algorithms since we want to analyse the time it takes to obtain the optimal solution. Heuristic algorithms typically can not ascertain whether the obtained solution is optimal and are therefore excluded. There are 

look at the amount of data that we have for each instance set / algorithm --> only creemers algorithm will work.

## Cure model

Mention that Time=0.00000001 if it is actually 0
```{r fit-models}
data_cv_all_1 <- data_cv_all %>%
  group_by(ID, author) %>%
  summarize(is_optimal = any(Type == "optimal"),
            Time = mean(Time)) %>%
  ungroup() %>%
  mutate(Time = ifelse(is_optimal,
                       Time,
                       ifelse(author == "C",
                              48*60*60*1000,
                              ifelse(author == "WZ",
                                     3600000,
                                     20*60*60*1000))
                       ),
         status = 1*is_optimal) %>%
  filter(author == "C") %>%
  left_join(data_cv, by = join_by(ID == IDset)) %>%
  mutate(Time = ifelse(Time <= 0.00001, 0.00001, Time)) %>%
  mutate(Time == 172800000, 126755558, Time) #lower the upper limit of the Time
data_cheat <- data_cv_all_1 %>%
  mutate(status = ifelse(Time > 5 * 10^5, 0, status),
         Time = ifelse(Time > 5 * 10^5, 5*10^5, Time))
covariates <- data_cv %>%
  dplyr::select(c(6:38, 58:61)) %>%
  dplyr::select(where(is.numeric)) %>%
  colnames()
covariates <- covariates[!str_detect(covariates, "-") &
                           !(covariates %in% c("RS", "RF", "nbR", "nbhR", "hRU", "hRC", "RC", "LA"))]
fm <- paste(covariates, collapse = " + ")
surv_fm <- as.formula(sprintf("~ %s", fm))
cure_fm <- surv_fm
fit <- cox_cure_net(surv_fm, cure_fm, data = data_cv_all_1,
                     time = Time, event = status,
                     surv_alpha = 0.5, cure_alpha = 0.5)
#This is the only model that yields SOMETHING for the surv, if surv_alpha>0, all coefficients are always zero:
fit <- cox_cure_net(~nbAct + CNC, cure_fm, data = data_cv_all_1,
                     time = (log(Time)+0.01), event = status,
                     surv_alpha = 0, cure_alpha = 0.8)
#blacklist: Wall, Wprec-> do seem relevant in the exploratory graphs

#This works though: 
  fit <- cox_cure_net(~nbAct + CNC + OS+SP+AD+I5+TF+Wprec+(FS31 >0.22) + LE, cure_fm, data = data_cheat,
                     time = (log(Time)+0.01), event = status,
                     surv_alpha = 0.1, cure_alpha = 0.8)
```

The prediction accuracy of the model is assessed using the concordance index that accommodates right-censored cure models [@asano2017assessing].

# Conclusion and future research

## CV dataset
The Coelho-Vanhoucke (CV) dataset consists of 623 RCPSP instances, first described in @cv2020 that can be downloaded from [here](https://www.projectmanagement.ugent.be/research/data).
Figure \@ref(fig:cv-cormat) shows the correlation between different parameters (Table \@ref(tab:parameter)) in all 623 instances.
Figure \@ref(fig: cv-cormat-reorder) shows the same correlation matrix with the rows and column reordered to show highly correlated hierarchical clusters.

As of September 2023, three different authors have submitted their run-time results and best (or optimal) solution to each of the instances;
- @cv2020 (CV) designed this dataset with difficult to solve instances and provided their heuristic solutions to some of them.
- @watermeyer2022partition (WZ) was the first one to add further solutions to improve upon the already achieved results.
- @creemers2021 (C) recently added his results and is able to solve almost all instances.


In this chapter, we compare the run times reported by each of the researchers for each of the instances. If a researcher is unable to find the optimal solution (within a certain time limit), they might still report the best solution that was found.


### Run times: Kaplan Meier

In this chapter, we look at the different run times to find an optimal solution. If an optimal solution is not found, a lower bound and/or heuristic solution was reported instead. If a non-optimal solution is reported, we consider the run time to be right-censored.

Figure \@ref(fig:cv-compare-authors) shows the run times for different authors. It can be seen than @cv2020 solves none of the instances to optimality. They designed this dataset to be difficult such that it can serve as a benchmarking dataset for new RCPSP algorithms. @watermeyer2022partition can solve almost 30% of all instances to optimality. Since they stop their algorithm after 3600 seconds, all other instances are right censored at that time. Lastly, @creemers2021 allows for a maximum run time of 48 hours. If no optimal solution is found after that, the instance is right-censored. 

Table \@ref(tab:cv-compare-authors-quantiles) shows different quantiles for each author with log-log 95% confidence intervals.

```{r cv-compare-authors, fig.cap="Comparing the run time for the different authors.", echo = FALSE, message = FALSE, warning = FALSE}
data_cv_all_1 <- data_cv_all %>%
  group_by(ID, author) %>%
  summarize(is_optimal = any(Type == "optimal"),
            Time = mean(Time)) %>%
  ungroup() %>%
  mutate(Time = ifelse(is_optimal,
                       Time,
                       ifelse(author == "C",
                              48*60*60*1000,
                              ifelse(author == "WZ",
                                     3600000,
                                     20*60*60*1000))
                       ),
         status = 1*is_optimal)
s1 <- survfit(Surv(Time, status) ~ author, data = data_cv_all_1)
survfit2(Surv(Time, status) ~ author, data = data_cv_all_1) %>%
  ggsurvfit() +
  labs(
    x = "milliseconds",
    y = "Proportion of instances not solved to optimality"
    )
```

```{r cv-compare-authors-quantiles, fig.cap="Comparing the run time for the different authors.", echo = FALSE, message = FALSE, warning = FALSE}
library(biostatUZH)
qKM10 <- quantileKM(data_cv_all_1$Time, data_cv_all_1$status,
                    group = data_cv_all_1$author, quant = 0.1,
                    conf.level = 0.95, conf.type = "log-log")
qKM25 <- quantileKM(data_cv_all_1$Time, data_cv_all_1$status,
                    group = data_cv_all_1$author, quant = 0.25,
                    conf.level = 0.95, conf.type = "log-log")
qKM50 <- quantileKM(data_cv_all_1$Time, data_cv_all_1$status,
                    group = data_cv_all_1$author, quant = 0.5,
                    conf.level = 0.95, conf.type = "log-log")
qKM75 <- quantileKM(data_cv_all_1$Time, data_cv_all_1$status, 
                  group = data_cv_all_1$author, quant = 0.75,
                  conf.level = 0.95, conf.type = "log-log")
qKM90 <- quantileKM(data_cv_all_1$Time, data_cv_all_1$status,
                    group = data_cv_all_1$author, quant = 0.9,
                    conf.level = 0.95, conf.type = "log-log")

qKM <- data_frame(author = c("CV", "WZ", "C"),
                  q90 = qKM90$quantities %>%
                    as.data.frame() %>%
                    mutate(q = sprintf("%.0f, [%.0f, %.0f]", quantile,
                                       lower.ci, upper.ci)) %>% dplyr::pull(q),
                  
                  q75 = qKM75$quantities %>%
                    as.data.frame() %>%
                    mutate(q = sprintf("%.0f, [%.0f, %.0f]", quantile,
                                       lower.ci, upper.ci)) %>% dplyr::pull(q),

                  q50 = qKM50$quantities %>%
                    as.data.frame() %>%
                    mutate(q = sprintf("%.0f, [%.0f, %.0f]", quantile,
                                       lower.ci, upper.ci)) %>% dplyr::pull(q),

                  q25 = qKM25$quantities %>%
                    as.data.frame() %>%
                    mutate(q = sprintf("%.0f, [%.0f, %.0f]", quantile,
                                       lower.ci, upper.ci)) %>% dplyr::pull(q),

                  q10 = qKM10$quantities %>%
                    as.data.frame() %>%
                    mutate(q = sprintf("%.0f, [%.0f, %.0f]", quantile,
                                       lower.ci, upper.ci)) %>% dplyr::pull(q))
qKM %>%
  kable(caption = "Quantiles and confidence intervals for the time to solve to optimality.",
        booktabs = TRUE) %>%
  kableExtra::kable_styling() %>%
  kableExtra::column_spec(1, width = "0.4in") %>%
  kableExtra::column_spec(2:6, width = "1in")
```

# Bibliography

<div id="refs"></div>

# (APPENDIX) Appendix {-} 

# Correlation between covariates {#app-1}

The following two figures show the correlation between the complexity parameters in the CV dataset. Figure \@ref(fig:cv-cormat-reorder) clearly shows that there are groups of covariates that are highly correlated. The correlation between RC and hRC, for example is `r round(M["RC", "hRC"],2)` while the correlation between RC and FS22 is `r round(M["RC", "FS22"],2)`.

```{r cv-cormat, echo = FALSE, message = FALSE, warning = FALSE, fig.cap = "Correlation between the different numeric parameters of interest.", fig.width = 9, fig.height = 6}

#corrplot(M)
order.AOE <- corrMatOrder(M, order = 'AOE')
order.FPC <- corrMatOrder(M, order = 'FPC')
order.hc <- corrMatOrder(M, order = 'hclust')
order.hc2 <- corrMatOrder(M, order = 'hclust', hclust.method = 'ward.D')

M.AOE <- M[order.AOE, order.AOE]
M.FPC <- M[order.FPC, order.FPC]
M.hc  <- M[order.hc, order.hc]
M.hc2 <- M[order.hc2, order.hc2]
corrplot(M)
#corrplot(M.AOE)
#corrplot(M.FPC)
#corrplot(M.hc)
```

```{r cv-cormat-reorder, echo = FALSE, message = FALSE, warning = FALSE, fig.cap = "Correlation between the different numeric parameters of interest (hierarchical clustering reordering).", fig.width = 9, fig.height = 6}
corrplot(M.hc)
corrRect.hclust(M.hc2, k = 6)
```

# Checking the proportional hazard assumption

Since we are using a Cox model, it is important to know whether the Cox proportional hazard assumption holds.
In this chapter, we explore the different parameters and how they influence the survival time.
Since all parameters are continuous variables, we split the instance set into three or four equally-sized groups where the parameter varies from low to high.
The Kaplan Meier curves will allow us to look for patterns that might uncover important covariates to predict runtimes.
Additionally, the figures allow us to visually check the PH assumption.

!!!??? also formally test PH assumption ???!!!
http://www.sthda.com/english/wiki/cox-model-assumptions#:~:text=The%20proportional%20hazards%20(PH)%20assumption,violation%20of%20the%20PH%20assumption.



```{r display-parameter-analysis, results = "asis", echo = FALSE, eval = TRUE}
#c("CNC", "AD", "TF", "Wprec", "RF", "RU", "RS", "RC")
to_do <- data_cv %>%
  dplyr::select(c(6:38, 58:61)) %>%
  dplyr::select(where(is.numeric)) %>%
  colnames()
rmd <- sapply(
  to_do,
  function(id) {
    knit_expand("_cv_analysis_parameter.Rmd", id = id)
  }
) %>%
  paste(collapse = "\n\n")
cat(knit(text = rmd, quiet = TRUE))
```
